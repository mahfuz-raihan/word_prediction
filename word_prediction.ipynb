{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import re # to handle regular expression\n",
    "import demoji  # for emojis handle\n",
    "import random # to generate random number\n",
    "import inflect  # to handle number to words\n",
    "import numpy as np # scitific calculation \n",
    "import pandas as pd # data manipulation \n",
    "from bs4 import BeautifulSoup # handle html tag\n",
    "import matplotlib.pyplot as plt # plot any display\n",
    "\n",
    "import torch # deep learning handle\n",
    "import torchtext # nlp handle\n",
    "import torch.nn as nn # NN handle\n",
    "import torch.optim as optim # optimizer handle\n",
    "import torch.nn.functional as F # all type to DL funciton\n",
    "from torch.nn.functional import one_hot # encoder-decoder\n",
    "from torchtext.data.utils import get_tokenizer # tokenize \n",
    "print('Successfully import all the libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the text \n",
    "file_path = 'human_chat.txt'\n",
    "with open(file_path,\"r\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(str(lines), 'html.parser')\n",
    "text = soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "        # Remove HTML tags\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "\n",
    "        p = inflect.engine() #101\n",
    "        # demoji.download_codes()\n",
    "        # Remove emojis\n",
    "        text = demoji.replace(text, \"\")\n",
    "\n",
    "        # Remove mentions of \"Human 1\" and \"Human 2\"\n",
    "        text = re.sub(r'\\b(?:Human 1|Human 2)\\b:?', \" \", text)\n",
    "\n",
    "        # Replace numbers with words\n",
    "        text = re.sub(r'\\b\\d+\\b', lambda x: p.number_to_words(x.group()), text)\n",
    "\n",
    "        # Remove special characters, keeping only alphabetic and spaces\n",
    "        text = re.sub('[^a-zA-Z\\s]', ' ', text)\n",
    "\n",
    "        # Replace specific unicode spaces with standard spaces and trim\n",
    "        text = text.replace(u'\\xa0', u' ').replace('\\u200a', ' ').strip()\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11338/3061531194.py:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "preprocessed_lines = [preprocess_text(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'What is your favorite holiday',\n",
       " 'one where I get to meet lots of different people',\n",
       " 'What was the most number of people you have ever met during a holiday',\n",
       " 'Hard to keep a count  Maybe twenty five']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_lines[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the words\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "tokenized_conv = [tokenizer(conv) for conv in preprocessed_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one', 'where', 'i', 'get', 'to', 'meet', 'lots', 'of', 'different', 'people']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_conv[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'was',\n",
       " 'the',\n",
       " 'most',\n",
       " 'number',\n",
       " 'of',\n",
       " 'people',\n",
       " 'you',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'met',\n",
       " 'during',\n",
       " 'a',\n",
       " 'holiday']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_conv[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'is', 'your', 'favorite', 'holiday']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_conv[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ```build_vocab_from_iterator```\n",
    "\n",
    "```build_vocab_from_iterator``` function in the ```torchtext.vocab``` modeule is used to create vocabulary from an iterable of tokenized data. This vocabulary is essential for converting textual data into numerical form.\n",
    "\n",
    "## Parameters:\n",
    "\n",
    "### ```tokenized_conv(iterator)```:\n",
    "This is the main data input to the function. It should be an iterator(like a ```generator``` or a ```list```) that yields sequences to tokens. Each sequences represents a document or an example in your dataset.\n",
    "\n",
    "### ```min_freq(int,optional):```\n",
    "This parameter specifics the mimimum frequency a token must have to be inclued in the vocabulary. Tokens that appear fewer than ```min_freq``` times are excluded form the vocabulary. This is useful for removing rare words which might be typos or irrelevant to most analysis.\n",
    "\n",
    "### ```specials(list of str, optional)```:\n",
    "This is a list of special tokens that you want to add to the vocabulary. Common special tokens include: A padding token used to equalize the lenghts of sequence.(or for unknown): A token used to represent out-of-vocabulary words during inference, or when a word appears that is not int the traning vocabualry. \n",
    "\n",
    "### ```special_first(bool, optional)```:\n",
    "Determine the ordering of special tokens in the vocabulary. If ```True```, special tokens are added at the beginning of the vocabulary. This can be helpful for certain models where token indices are significant(e.g., models using embadding layers might have specific handling for lower indices.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    tokenized_conv,\n",
    "    min_freq=1,\n",
    "    specials=['<pad>', '<oov>'],\n",
    "    special_first=True\n",
    ")\n",
    "target_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    tokenized_conv,\n",
    "    min_freq=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Vocab Length: 2749\n",
      "Features Vocab Length: 2747\n"
     ]
    }
   ],
   "source": [
    "features_vocab_total_words = len(features_vocab)\n",
    "target_vocab_toal_words = len(target_vocab)\n",
    "print(\"Features Vocab Length:\", features_vocab_total_words)\n",
    "print(\"Features Vocab Length:\", target_vocab_toal_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making ngrams from the conversations\n",
    "def make_ngrams(tokenized_text):\n",
    "    list_ngrams = []\n",
    "    for i in range(1, len(tokenized_text)):\n",
    "        ngram_sequence = tokenized_text[:i+1]\n",
    "        list_ngrams.append(ngram_sequence)\n",
    "    return list_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_list = []\n",
    "for tokenized_con in tokenized_conv:\n",
    "    ngrams_list.extend(make_ngrams(tokenized_con))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['what', 'is', 'your']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_list[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m t2 \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m t:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mt2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(t2)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "t2 = []\n",
    "for i in t:\n",
    "    t2.extend(i)\n",
    "print(t2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
